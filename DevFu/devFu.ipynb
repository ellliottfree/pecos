{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmc-base/eurlex-4k/X.trn.txt\n",
      "xmc-base/eurlex-4k/X.tst.txt\n",
      "xmc-base/eurlex-4k/Y.trn.npz\n",
      "xmc-base/eurlex-4k/Y.trn.txt\n",
      "xmc-base/eurlex-4k/Y.tst.npz\n",
      "xmc-base/eurlex-4k/Y.tst.txt\n",
      "xmc-base/eurlex-4k/output-items.txt\n",
      "xmc-base/eurlex-4k/tfidf-attnxml\n",
      "xmc-base/eurlex-4k/tfidf-attnxml/X.tst.npz\n",
      "xmc-base/eurlex-4k/tfidf-attnxml/X.trn.npz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATASET=\"eurlex-4k\"\n",
    "wget -nv -nc https://archive.org/download/pecos-dataset/xmc-base/${DATASET}.tar.gz\n",
    "tar --skip-old-files -zxf ${DATASET}.tar.gz \n",
    "find xmc-base/${DATASET}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from pecos.utils import smat_util, logging_util\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# set logging level to WARNING(1)\n",
    "# you can change this to INFO(2) or DEBUG(3) if you'd like to see more logging\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "logging_util.setup_logging_config(level=1)\n",
    "\n",
    "# load training data\n",
    "X_feat_trn = smat_util.load_matrix(\"xmc-base/eurlex-4k/tfidf-attnxml/X.trn.npz\", dtype=np.float32)\n",
    "Y_trn = smat_util.load_matrix(\"xmc-base/eurlex-4k/Y.trn.npz\", dtype=np.float32)\n",
    "\n",
    "with open(\"xmc-base/eurlex-4k/X.trn.txt\", 'r') as fin:\n",
    "    X_txt_trn = [xx.strip() for xx in fin.readlines()]\n",
    "\n",
    "# load test data\n",
    "X_feat_tst = smat_util.load_matrix(\"xmc-base/eurlex-4k/tfidf-attnxml/X.tst.npz\", dtype=np.float32)\n",
    "Y_tst = smat_util.load_matrix(\"xmc-base/eurlex-4k/Y.tst.npz\", dtype=np.float32)\n",
    "\n",
    "with open(\"xmc-base/eurlex-4k/X.tst.txt\", 'r') as fin:\n",
    "    X_txt_tst = [xx.strip() for xx in fin.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTransformer.TrainParams(preliminary_indexer_params=HierarchicalKMeans.TrainParams(nr_splits=16, min_codes=16, max_leaf_size=16, spherical=True, seed=0, kmeans_max_iter=20, threads=-1), refined_indexer_params=HierarchicalKMeans.TrainParams(nr_splits=8, min_codes=None, max_leaf_size=16, spherical=True, seed=0, kmeans_max_iter=20, threads=-1), matcher_params_chain=[TransformerMatcher.TrainParams(model_shortcut='bert-base-uncased', negative_sampling='tfn+man', loss_function='weighted-squared-hinge', bootstrap_method='weighted-linear', lr_schedule='linear', threshold=0.001, hidden_dropout_prob=0.1, batch_size=32, batch_gen_workers=16, max_active_matching_labels=1000, max_num_labels_in_gpu=65536, max_steps=600, max_no_improve_cnt=-1, num_train_epochs=10, gradient_accumulation_steps=1, weight_decay=0.0, max_grad_norm=1.0, learning_rate=5e-05, adam_epsilon=1e-08, warmup_steps=100, logging_steps=50, save_steps=200, cost_sensitive_ranker=False, pre_tokenize=True, pre_tensorize_labels=True, use_gpu=True, eval_by_true_shorlist=False, checkpoint_dir='', cache_dir='', init_model_dir=''), TransformerMatcher.TrainParams(model_shortcut='bert-base-uncased', negative_sampling='tfn+man', loss_function='weighted-squared-hinge', bootstrap_method='weighted-linear', lr_schedule='linear', threshold=0.001, hidden_dropout_prob=0.1, batch_size=32, batch_gen_workers=16, max_active_matching_labels=1000, max_num_labels_in_gpu=65536, max_steps=800, max_no_improve_cnt=-1, num_train_epochs=10, gradient_accumulation_steps=1, weight_decay=0.0, max_grad_norm=1.0, learning_rate=5e-05, adam_epsilon=1e-08, warmup_steps=100, logging_steps=50, save_steps=200, cost_sensitive_ranker=False, pre_tokenize=True, pre_tensorize_labels=True, use_gpu=True, eval_by_true_shorlist=False, checkpoint_dir='', cache_dir='', init_model_dir=''), TransformerMatcher.TrainParams(model_shortcut='bert-base-uncased', negative_sampling='tfn+man', loss_function='weighted-squared-hinge', bootstrap_method='weighted-linear', lr_schedule='linear', threshold=0.001, hidden_dropout_prob=0.1, batch_size=32, batch_gen_workers=16, max_active_matching_labels=1000, max_num_labels_in_gpu=65536, max_steps=1200, max_no_improve_cnt=-1, num_train_epochs=10, gradient_accumulation_steps=1, weight_decay=0.0, max_grad_norm=1.0, learning_rate=5e-05, adam_epsilon=1e-08, warmup_steps=100, logging_steps=50, save_steps=200, cost_sensitive_ranker=False, pre_tokenize=True, pre_tensorize_labels=True, use_gpu=True, eval_by_true_shorlist=False, checkpoint_dir='', cache_dir='', init_model_dir='')], ranker_params=XLinearModel.TrainParams(mode='full-model', ranker_level=1, nr_splits=16, min_codes=None, shallow=False, rel_mode='induce', rel_norm='l1', hlm_args=HierarchicalMLModel.TrainParams(neg_mining_chain=['tfn', 'tfn', 'tfn', 'tfn+man'], model_chain=[MLModel.TrainParams(threshold=0.001, max_nonzeros_per_label=None, solver_type='L2R_L2LOSS_SVC_DUAL', Cp=1.0, Cn=2.0, max_iter=100, eps=0.1, bias=1.0, threads=-1, verbose=0, newton_eps=0.01), MLModel.TrainParams(threshold=0.001, max_nonzeros_per_label=None, solver_type='L2R_L2LOSS_SVC_DUAL', Cp=1.0, Cn=2.0, max_iter=100, eps=0.1, bias=1.0, threads=-1, verbose=0, newton_eps=0.01), MLModel.TrainParams(threshold=0.001, max_nonzeros_per_label=None, solver_type='L2R_L2LOSS_SVC_DUAL', Cp=1.0, Cn=2.0, max_iter=100, eps=0.1, bias=1.0, threads=-1, verbose=0, newton_eps=0.01), MLModel.TrainParams(threshold=0.001, max_nonzeros_per_label=None, solver_type='L2R_L2LOSS_SVC_DUAL', Cp=2.0, Cn=4.0, max_iter=100, eps=0.1, bias=1.0, threads=-1, verbose=0, newton_eps=0.01)])), do_fine_tune=True, only_encoder=False, fix_clustering=False, max_match_clusters=32768)\n",
      "{\n",
      " \"__meta__\": {\n",
      "  \"class_fullname\": \"pecos.xmc.xtransformer.model###XTransformer.TrainParams\"\n",
      " },\n",
      " \"preliminary_indexer_params\": {\n",
      "  \"__meta__\": {\n",
      "   \"class_fullname\": \"pecos.xmc.base###HierarchicalKMeans.TrainParams\"\n",
      "  },\n",
      "  \"nr_splits\": 16,\n",
      "  \"min_codes\": 16,\n",
      "  \"max_leaf_size\": 16,\n",
      "  \"spherical\": true,\n",
      "  \"seed\": 0,\n",
      "  \"kmeans_max_iter\": 20,\n",
      "  \"threads\": -1\n",
      " },\n",
      " \"refined_indexer_params\": {\n",
      "  \"__meta__\": {\n",
      "   \"class_fullname\": \"pecos.xmc.base###HierarchicalKMeans.TrainParams\"\n",
      "  },\n",
      "  \"nr_splits\": 8,\n",
      "  \"min_codes\": null,\n",
      "  \"max_leaf_size\": 16,\n",
      "  \"spherical\": true,\n",
      "  \"seed\": 0,\n",
      "  \"kmeans_max_iter\": 20,\n",
      "  \"threads\": -1\n",
      " },\n",
      " \"matcher_params_chain\": [\n",
      "  {\n",
      "   \"__meta__\": {\n",
      "    \"class_fullname\": \"pecos.xmc.xtransformer.matcher###TransformerMatcher.TrainParams\"\n",
      "   },\n",
      "   \"model_shortcut\": \"bert-base-uncased\",\n",
      "   \"negative_sampling\": \"tfn+man\",\n",
      "   \"loss_function\": \"weighted-squared-hinge\",\n",
      "   \"bootstrap_method\": \"weighted-linear\",\n",
      "   \"lr_schedule\": \"linear\",\n",
      "   \"threshold\": 0.001,\n",
      "   \"hidden_dropout_prob\": 0.1,\n",
      "   \"batch_size\": 32,\n",
      "   \"batch_gen_workers\": 16,\n",
      "   \"max_active_matching_labels\": 1000,\n",
      "   \"max_num_labels_in_gpu\": 65536,\n",
      "   \"max_steps\": 600,\n",
      "   \"max_no_improve_cnt\": -1,\n",
      "   \"num_train_epochs\": 10,\n",
      "   \"gradient_accumulation_steps\": 1,\n",
      "   \"weight_decay\": 0.0,\n",
      "   \"max_grad_norm\": 1.0,\n",
      "   \"learning_rate\": 5e-05,\n",
      "   \"adam_epsilon\": 1e-08,\n",
      "   \"warmup_steps\": 100,\n",
      "   \"logging_steps\": 50,\n",
      "   \"save_steps\": 200,\n",
      "   \"cost_sensitive_ranker\": false,\n",
      "   \"pre_tokenize\": true,\n",
      "   \"pre_tensorize_labels\": true,\n",
      "   \"use_gpu\": true,\n",
      "   \"eval_by_true_shorlist\": false,\n",
      "   \"checkpoint_dir\": \"\",\n",
      "   \"cache_dir\": \"\",\n",
      "   \"init_model_dir\": \"\"\n",
      "  },\n",
      "  {\n",
      "   \"__meta__\": {\n",
      "    \"class_fullname\": \"pecos.xmc.xtransformer.matcher###TransformerMatcher.TrainParams\"\n",
      "   },\n",
      "   \"model_shortcut\": \"bert-base-uncased\",\n",
      "   \"negative_sampling\": \"tfn+man\",\n",
      "   \"loss_function\": \"weighted-squared-hinge\",\n",
      "   \"bootstrap_method\": \"weighted-linear\",\n",
      "   \"lr_schedule\": \"linear\",\n",
      "   \"threshold\": 0.001,\n",
      "   \"hidden_dropout_prob\": 0.1,\n",
      "   \"batch_size\": 32,\n",
      "   \"batch_gen_workers\": 16,\n",
      "   \"max_active_matching_labels\": 1000,\n",
      "   \"max_num_labels_in_gpu\": 65536,\n",
      "   \"max_steps\": 800,\n",
      "   \"max_no_improve_cnt\": -1,\n",
      "   \"num_train_epochs\": 10,\n",
      "   \"gradient_accumulation_steps\": 1,\n",
      "   \"weight_decay\": 0.0,\n",
      "   \"max_grad_norm\": 1.0,\n",
      "   \"learning_rate\": 5e-05,\n",
      "   \"adam_epsilon\": 1e-08,\n",
      "   \"warmup_steps\": 100,\n",
      "   \"logging_steps\": 50,\n",
      "   \"save_steps\": 200,\n",
      "   \"cost_sensitive_ranker\": false,\n",
      "   \"pre_tokenize\": true,\n",
      "   \"pre_tensorize_labels\": true,\n",
      "   \"use_gpu\": true,\n",
      "   \"eval_by_true_shorlist\": false,\n",
      "   \"checkpoint_dir\": \"\",\n",
      "   \"cache_dir\": \"\",\n",
      "   \"init_model_dir\": \"\"\n",
      "  },\n",
      "  {\n",
      "   \"__meta__\": {\n",
      "    \"class_fullname\": \"pecos.xmc.xtransformer.matcher###TransformerMatcher.TrainParams\"\n",
      "   },\n",
      "   \"model_shortcut\": \"bert-base-uncased\",\n",
      "   \"negative_sampling\": \"tfn+man\",\n",
      "   \"loss_function\": \"weighted-squared-hinge\",\n",
      "   \"bootstrap_method\": \"weighted-linear\",\n",
      "   \"lr_schedule\": \"linear\",\n",
      "   \"threshold\": 0.001,\n",
      "   \"hidden_dropout_prob\": 0.1,\n",
      "   \"batch_size\": 32,\n",
      "   \"batch_gen_workers\": 16,\n",
      "   \"max_active_matching_labels\": 1000,\n",
      "   \"max_num_labels_in_gpu\": 65536,\n",
      "   \"max_steps\": 1200,\n",
      "   \"max_no_improve_cnt\": -1,\n",
      "   \"num_train_epochs\": 10,\n",
      "   \"gradient_accumulation_steps\": 1,\n",
      "   \"weight_decay\": 0.0,\n",
      "   \"max_grad_norm\": 1.0,\n",
      "   \"learning_rate\": 5e-05,\n",
      "   \"adam_epsilon\": 1e-08,\n",
      "   \"warmup_steps\": 100,\n",
      "   \"logging_steps\": 50,\n",
      "   \"save_steps\": 200,\n",
      "   \"cost_sensitive_ranker\": false,\n",
      "   \"pre_tokenize\": true,\n",
      "   \"pre_tensorize_labels\": true,\n",
      "   \"use_gpu\": true,\n",
      "   \"eval_by_true_shorlist\": false,\n",
      "   \"checkpoint_dir\": \"\",\n",
      "   \"cache_dir\": \"\",\n",
      "   \"init_model_dir\": \"\"\n",
      "  }\n",
      " ],\n",
      " \"ranker_params\": {\n",
      "  \"__meta__\": {\n",
      "   \"class_fullname\": \"pecos.xmc.xlinear.model###XLinearModel.TrainParams\"\n",
      "  },\n",
      "  \"mode\": \"full-model\",\n",
      "  \"ranker_level\": 1,\n",
      "  \"nr_splits\": 16,\n",
      "  \"min_codes\": null,\n",
      "  \"shallow\": false,\n",
      "  \"rel_mode\": \"induce\",\n",
      "  \"rel_norm\": \"l1\",\n",
      "  \"hlm_args\": {\n",
      "   \"__meta__\": {\n",
      "    \"class_fullname\": \"pecos.xmc.base###HierarchicalMLModel.TrainParams\"\n",
      "   },\n",
      "   \"neg_mining_chain\": [\n",
      "    \"tfn\",\n",
      "    \"tfn\",\n",
      "    \"tfn\",\n",
      "    \"tfn+man\"\n",
      "   ],\n",
      "   \"model_chain\": [\n",
      "    {\n",
      "     \"__meta__\": {\n",
      "      \"class_fullname\": \"pecos.xmc.base###MLModel.TrainParams\"\n",
      "     },\n",
      "     \"threshold\": 0.001,\n",
      "     \"max_nonzeros_per_label\": null,\n",
      "     \"solver_type\": \"L2R_L2LOSS_SVC_DUAL\",\n",
      "     \"Cp\": 1.0,\n",
      "     \"Cn\": 2.0,\n",
      "     \"max_iter\": 100,\n",
      "     \"eps\": 0.1,\n",
      "     \"bias\": 1.0,\n",
      "     \"threads\": -1,\n",
      "     \"verbose\": 0,\n",
      "     \"newton_eps\": 0.01\n",
      "    },\n",
      "    {\n",
      "     \"__meta__\": {\n",
      "      \"class_fullname\": \"pecos.xmc.base###MLModel.TrainParams\"\n",
      "     },\n",
      "     \"threshold\": 0.001,\n",
      "     \"max_nonzeros_per_label\": null,\n",
      "     \"solver_type\": \"L2R_L2LOSS_SVC_DUAL\",\n",
      "     \"Cp\": 1.0,\n",
      "     \"Cn\": 2.0,\n",
      "     \"max_iter\": 100,\n",
      "     \"eps\": 0.1,\n",
      "     \"bias\": 1.0,\n",
      "     \"threads\": -1,\n",
      "     \"verbose\": 0,\n",
      "     \"newton_eps\": 0.01\n",
      "    },\n",
      "    {\n",
      "     \"__meta__\": {\n",
      "      \"class_fullname\": \"pecos.xmc.base###MLModel.TrainParams\"\n",
      "     },\n",
      "     \"threshold\": 0.001,\n",
      "     \"max_nonzeros_per_label\": null,\n",
      "     \"solver_type\": \"L2R_L2LOSS_SVC_DUAL\",\n",
      "     \"Cp\": 1.0,\n",
      "     \"Cn\": 2.0,\n",
      "     \"max_iter\": 100,\n",
      "     \"eps\": 0.1,\n",
      "     \"bias\": 1.0,\n",
      "     \"threads\": -1,\n",
      "     \"verbose\": 0,\n",
      "     \"newton_eps\": 0.01\n",
      "    },\n",
      "    {\n",
      "     \"__meta__\": {\n",
      "      \"class_fullname\": \"pecos.xmc.base###MLModel.TrainParams\"\n",
      "     },\n",
      "     \"threshold\": 0.001,\n",
      "     \"max_nonzeros_per_label\": null,\n",
      "     \"solver_type\": \"L2R_L2LOSS_SVC_DUAL\",\n",
      "     \"Cp\": 2.0,\n",
      "     \"Cn\": 4.0,\n",
      "     \"max_iter\": 100,\n",
      "     \"eps\": 0.1,\n",
      "     \"bias\": 1.0,\n",
      "     \"threads\": -1,\n",
      "     \"verbose\": 0,\n",
      "     \"newton_eps\": 0.01\n",
      "    }\n",
      "   ]\n",
      "  }\n",
      " },\n",
      " \"do_fine_tune\": true,\n",
      " \"only_encoder\": false,\n",
      " \"fix_clustering\": false,\n",
      " \"max_match_clusters\": 32768\n",
      "}\n",
      "{\n",
      " \"__meta__\": {\n",
      "  \"class_fullname\": \"pecos.xmc.xtransformer.model###XTransformer.PredParams\"\n",
      " },\n",
      " \"matcher_params_chain\": [\n",
      "  {\n",
      "   \"__meta__\": {\n",
      "    \"class_fullname\": \"pecos.xmc.xtransformer.matcher###TransformerMatcher.PredParams\"\n",
      "   },\n",
      "   \"only_topk\": 5,\n",
      "   \"post_processor\": \"l3-hinge\",\n",
      "   \"ensemble_method\": \"concat-only\",\n",
      "   \"truncate_length\": 128\n",
      "  },\n",
      "  {\n",
      "   \"__meta__\": {\n",
      "    \"class_fullname\": \"pecos.xmc.xtransformer.matcher###TransformerMatcher.PredParams\"\n",
      "   },\n",
      "   \"only_topk\": 5,\n",
      "   \"post_processor\": \"l3-hinge\",\n",
      "   \"ensemble_method\": \"concat-only\",\n",
      "   \"truncate_length\": 128\n",
      "  },\n",
      "  {\n",
      "   \"__meta__\": {\n",
      "    \"class_fullname\": \"pecos.xmc.xtransformer.matcher###TransformerMatcher.PredParams\"\n",
      "   },\n",
      "   \"only_topk\": 5,\n",
      "   \"post_processor\": \"l3-hinge\",\n",
      "   \"ensemble_method\": \"concat-only\",\n",
      "   \"truncate_length\": 128\n",
      "  }\n",
      " ],\n",
      " \"ranker_params\": {\n",
      "  \"__meta__\": {\n",
      "   \"class_fullname\": \"pecos.xmc.xlinear.model###XLinearModel.PredParams\"\n",
      "  },\n",
      "  \"hlm_args\": {\n",
      "   \"__meta__\": {\n",
      "    \"class_fullname\": \"pecos.xmc.base###HierarchicalMLModel.PredParams\"\n",
      "   },\n",
      "   \"model_chain\": [\n",
      "    {\n",
      "     \"__meta__\": {\n",
      "      \"class_fullname\": \"pecos.xmc.base###MLModel.PredParams\"\n",
      "     },\n",
      "     \"only_topk\": 75,\n",
      "     \"post_processor\": \"l3-hinge\"\n",
      "    },\n",
      "    {\n",
      "     \"__meta__\": {\n",
      "      \"class_fullname\": \"pecos.xmc.base###MLModel.PredParams\"\n",
      "     },\n",
      "     \"only_topk\": 75,\n",
      "     \"post_processor\": \"l3-hinge\"\n",
      "    },\n",
      "    {\n",
      "     \"__meta__\": {\n",
      "      \"class_fullname\": \"pecos.xmc.base###MLModel.PredParams\"\n",
      "     },\n",
      "     \"only_topk\": 100,\n",
      "     \"post_processor\": \"l3-hinge\"\n",
      "    },\n",
      "    {\n",
      "     \"__meta__\": {\n",
      "      \"class_fullname\": \"pecos.xmc.base###MLModel.PredParams\"\n",
      "     },\n",
      "     \"only_topk\": 25,\n",
      "     \"post_processor\": \"noop\"\n",
      "    }\n",
      "   ]\n",
      "  }\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from pecos.xmc.xtransformer.model import XTransformer\n",
    "\n",
    "# get XR-Transformer training params\n",
    "\n",
    "params = json.load(open(\"params.json\"))\n",
    "\n",
    "    \n",
    "eurlex4k_train_params = XTransformer.TrainParams.from_dict(params[\"train_params\"])\n",
    "print(eurlex4k_train_params)\n",
    "eurlex4k_pred_params = XTransformer.PredParams.from_dict(params[\"pred_params\"])\n",
    "\n",
    "# you can view the detailed parameter setting via\n",
    "print(json.dumps(eurlex4k_train_params.to_dict(), indent=True))\n",
    "print(json.dumps(eurlex4k_pred_params.to_dict(), indent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics of XR-Linear model\n",
      "prec   = 85.05 77.98 71.30 64.77 58.78 53.15 48.02 43.61 39.94 36.84\n",
      "recall = 17.26 31.33 42.44 50.89 57.28 61.73 64.78 67.08 68.96 70.53\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def apply_uniform_noise(Y, noise_level):\n",
    "    Y_dense = Y.toarray()  # convert to dense matrix\n",
    "    Y_noisy = Y_dense.copy()\n",
    "    num_classes = Y_dense.shape[1]\n",
    "    for i in range(num_classes):\n",
    "        mask = np.random.rand(len(Y_dense)) < noise_level\n",
    "        Y_noisy[mask, i] = 1 - Y_dense[mask, i]  # flip the labels\n",
    "    return csr_matrix(Y_noisy)\n",
    "\n",
    "# Y_trn = apply_uniform_noise(Y_trn, 0.2)\n",
    "\n",
    "\n",
    "# construct label hierarchy\n",
    "from pecos.xmc import Indexer, LabelEmbeddingFactory\n",
    "cluster_chain = Indexer.gen(\n",
    "    LabelEmbeddingFactory.create(Y_trn, X_feat_trn, method=\"pifa\"),\n",
    "    train_params=eurlex4k_train_params.refined_indexer_params,\n",
    ")\n",
    "\n",
    "# train XR-Linear model\n",
    "from pecos.xmc.xlinear import XLinearModel\n",
    "xlm = XLinearModel.train(\n",
    "    X_feat_trn,\n",
    "    Y_trn,\n",
    "    C=cluster_chain,\n",
    "    train_params=eurlex4k_train_params.ranker_params,\n",
    "    pred_params=eurlex4k_pred_params.ranker_params,\n",
    ")\n",
    "\n",
    "# predict on test set with XR-Linear model\n",
    "P_xlm = xlm.predict(X_feat_tst)\n",
    "\n",
    "# compute metrics using ground truth\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xlm)\n",
    "print(\"Evaluation metrics of XR-Linear model\")\n",
    "print(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline 1: XR-Linear\n",
    "Let's train a XR-Linear model on the TF-IDF features using the same hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics of XR-Linear model\n",
      "prec   = 85.05 77.98 71.30 64.77 58.78 53.15 48.02 43.61 39.94 36.84\n",
      "recall = 17.26 31.33 42.44 50.89 57.28 61.73 64.78 67.08 68.96 70.53\n"
     ]
    }
   ],
   "source": [
    "# construct label hierarchy\n",
    "from pecos.xmc import Indexer, LabelEmbeddingFactory\n",
    "cluster_chain = Indexer.gen(\n",
    "    LabelEmbeddingFactory.create(Y_trn, X_feat_trn, method=\"pifa\"),\n",
    "    train_params=eurlex4k_train_params.refined_indexer_params,\n",
    ")\n",
    "\n",
    "# train XR-Linear model\n",
    "from pecos.xmc.xlinear import XLinearModel\n",
    "xlm = XLinearModel.train(\n",
    "    X_feat_trn,\n",
    "    Y_trn,\n",
    "    C=cluster_chain,\n",
    "    train_params=eurlex4k_train_params.ranker_params,\n",
    "    pred_params=eurlex4k_pred_params.ranker_params,\n",
    ")\n",
    "\n",
    "# predict on test set with XR-Linear model\n",
    "P_xlm = xlm.predict(X_feat_tst)\n",
    "\n",
    "# compute metrics using ground truth\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xlm)\n",
    "print(\"Evaluation metrics of XR-Linear model\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise  generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_uniform_noise(Y, noise_level):\n",
    "    Y_dense = Y.toarray()  # convert to dense matrix\n",
    "    Y_noisy = Y_dense.copy()\n",
    "    num_classes = Y_dense.shape[1]\n",
    "    for i in range(num_classes):\n",
    "        mask = np.random.rand(len(Y_dense)) < noise_level\n",
    "        Y_noisy[mask, i] = 1 - Y_dense[mask, i]  # flip the labels\n",
    "    return csr_matrix(Y_noisy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 224kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 7.81kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.04MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 792kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [00:22<00:00, 19.4MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics of XR-Transformer (not fine-tuned)\n",
      "prec   = 41.22 29.75 23.43 19.17 16.37 14.24 12.64 11.42 10.40 9.60\n",
      "recall = 8.27 11.74 13.69 14.91 15.88 16.54 17.12 17.66 18.09 18.54\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y_trn = apply_uniform_noise(Y_trn, 0.2)\n",
    "# define the problem\n",
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt_trn, Y_trn, X_feat=X_feat_trn)\n",
    "\n",
    "\n",
    "eurlex4k_train_params.do_fine_tune = True\n",
    "\n",
    "\n",
    "xrt_pretrained = XTransformer.train(\n",
    "    prob,\n",
    "    train_params=eurlex4k_train_params,\n",
    "    pred_params=eurlex4k_pred_params,\n",
    ")\n",
    "\n",
    "# predict and compute metrics\n",
    "P_xrt_pretrained = xrt_pretrained.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_pretrained)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"./work_dir/Uni20%\"\n",
    "xrt_pretrained.save(model_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pair_noise(Y, noise_level):\n",
    "    Y_dense = Y.toarray()  # Convert the sparse matrix to a dense matrix\n",
    "    Y_noisy = Y_dense.copy()\n",
    "    num_classes = Y_dense.shape[1]\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j:\n",
    "                mask = np.random.rand(len(Y_dense)) < noise_level\n",
    "                Y_noisy[mask & (Y_dense[:, i] == 1), i] = 0  # Flip class i labels to other class j\n",
    "                Y_noisy[mask & (Y_dense[:, i] == 0), i] = 1  # Flip other class j labels to class i\n",
    "    return csr_matrix(Y_noisy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "def apply_pair_noise(Y, noise_level):\n",
    "    Y_noisy = Y.copy()\n",
    "    num_classes = Y.shape[1]\n",
    "    \n",
    "    mask = np.random.rand(Y.shape[0]) < noise_level\n",
    "    mask = csr_matrix(mask[:, np.newaxis])  # 将掩码转换为稀疏矩阵\n",
    "    \n",
    "    Y_lil = Y_noisy.tolil()  # 将稀疏矩阵转换为LIL格式\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        flip_indices = mask.multiply(Y[:, i] == 0).nonzero()[0]  # 获取需要翻转的索引\n",
    "        Y_lil[flip_indices, i] = 1  # 翻转 i 到其他类别 j\n",
    "        \n",
    "        flip_indices = mask.multiply(Y[:, i] == 1).nonzero()[0]  # 获取需要翻转的索引\n",
    "        Y_lil[flip_indices, i] = 0  # 翻转其他类别 j 到 i\n",
    "    \n",
    "    return csr_matrix(Y_lil)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair flipping  \n",
    "noise_level: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2922/2465004771.py:1: SparseEfficiencyWarning: Comparing a sparse matrix with 0 using == is inefficient, try using != instead.\n",
      "  Y_trn = apply_pair_noise(Y_trn, 0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fd6627470d0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "05/16/2023 04:02:24 - WARNING - huggingface_hub.utils._http - 'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fd6627470d0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fd6627d1130>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "05/16/2023 04:10:35 - WARNING - huggingface_hub.utils._http - 'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fd6627d1130>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 80.65 74.01 67.47 61.35 55.69 50.45 45.64 41.49 38.00 35.03\n",
      "recall = 16.35 29.66 40.23 48.31 54.34 58.75 61.76 64.00 65.82 67.31\n"
     ]
    }
   ],
   "source": [
    "Y_trn = apply_pair_noise(Y_trn, 0.2)\n",
    "\n",
    "\n",
    "print('Noise applied')\n",
    "\n",
    "\n",
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt_trn, Y_trn, X_feat=X_feat_trn)\n",
    "\n",
    "\n",
    "eurlex4k_train_params.do_fine_tune = True\n",
    "\n",
    "\n",
    "xrt_pretrained = XTransformer.train(\n",
    "    prob,\n",
    "    train_params=eurlex4k_train_params,\n",
    "    pred_params=eurlex4k_pred_params,\n",
    ")\n",
    "\n",
    "# predict and compute metrics\n",
    "P_xrt_pretrained = xrt_pretrained.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_pretrained)\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "\n",
    "model_folder = \"./work_dir/Pair20%\"\n",
    "xrt_pretrained.save(model_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair flipping  \n",
    "noise_level: 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2922/2599280508.py:1: SparseEfficiencyWarning: Comparing a sparse matrix with 0 using == is inefficient, try using != instead.\n",
      "  Y_trn = apply_pair_noise(Y_trn, 0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fd6419aa8e0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "05/16/2023 04:53:24 - WARNING - huggingface_hub.utils._http - 'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fd6419aa8e0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 66.18 60.27 55.19 50.49 46.05 41.87 37.98 34.67 31.95 29.58\n",
      "recall = 13.38 24.04 32.70 39.56 44.76 48.65 51.35 53.44 55.28 56.79\n"
     ]
    }
   ],
   "source": [
    "Y_trn = apply_pair_noise(Y_trn, 0.4)\n",
    "\n",
    "\n",
    "print('Noise applied')\n",
    "\n",
    "\n",
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt_trn, Y_trn, X_feat=X_feat_trn)\n",
    "\n",
    "\n",
    "eurlex4k_train_params.do_fine_tune = True\n",
    "\n",
    "\n",
    "xrt_pretrained = XTransformer.train(\n",
    "    prob,\n",
    "    train_params=eurlex4k_train_params,\n",
    "    pred_params=eurlex4k_pred_params,\n",
    ")\n",
    "\n",
    "# predict and compute metrics\n",
    "P_xrt_pretrained = xrt_pretrained.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_pretrained)\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "\n",
    "model_folder = \"./work_dir/Pair40%\"\n",
    "xrt_pretrained.save(model_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair flipping  \n",
    "noise_level: 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2922/3862887829.py:1: SparseEfficiencyWarning: Comparing a sparse matrix with 0 using == is inefficient, try using != instead.\n",
      "  Y_trn = apply_pair_noise(Y_trn, 0.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 60.21 55.30 50.89 46.65 42.64 38.97 35.79 32.77 30.23 28.02\n",
      "recall = 12.11 22.05 30.23 36.70 41.64 45.45 48.53 50.67 52.50 54.01\n"
     ]
    }
   ],
   "source": [
    "Y_trn = apply_pair_noise(Y_trn, 0.6)\n",
    "\n",
    "\n",
    "print('Noise applied')\n",
    "\n",
    "\n",
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt_trn, Y_trn, X_feat=X_feat_trn)\n",
    "\n",
    "\n",
    "eurlex4k_train_params.do_fine_tune = True\n",
    "\n",
    "\n",
    "xrt_pretrained = XTransformer.train(\n",
    "    prob,\n",
    "    train_params=eurlex4k_train_params,\n",
    "    pred_params=eurlex4k_pred_params,\n",
    ")\n",
    "\n",
    "# predict and compute metrics\n",
    "P_xrt_pretrained = xrt_pretrained.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_pretrained)\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "\n",
    "model_folder = \"./work_dir/Pair60%\"\n",
    "xrt_pretrained.save(model_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair flipping  \n",
    "noise_level: 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2922/1777011111.py:1: SparseEfficiencyWarning: Comparing a sparse matrix with 0 using == is inefficient, try using != instead.\n",
      "  Y_trn = apply_pair_noise(Y_trn, 0.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fd6415e6fa0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "05/16/2023 07:11:30 - WARNING - huggingface_hub.utils._http - 'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fd6415e6fa0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 62.95 56.95 52.38 48.22 44.16 40.30 36.84 33.77 31.05 28.76\n",
      "recall = 12.68 22.69 31.07 37.82 43.04 46.91 49.81 52.05 53.72 55.22\n"
     ]
    }
   ],
   "source": [
    "Y_trn = apply_pair_noise(Y_trn, 0.8)\n",
    "\n",
    "\n",
    "print('Noise applied')\n",
    "\n",
    "\n",
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt_trn, Y_trn, X_feat=X_feat_trn)\n",
    "\n",
    "\n",
    "eurlex4k_train_params.do_fine_tune = True\n",
    "\n",
    "\n",
    "xrt_pretrained = XTransformer.train(\n",
    "    prob,\n",
    "    train_params=eurlex4k_train_params,\n",
    "    pred_params=eurlex4k_pred_params,\n",
    ")\n",
    "\n",
    "# predict and compute metrics\n",
    "P_xrt_pretrained = xrt_pretrained.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_pretrained)\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "\n",
    "model_folder = \"./work_dir/Pair80%\"\n",
    "xrt_pretrained.save(model_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Flipping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform flipping    \n",
    "noise_level: 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 0.28 0.35 0.33 0.29 0.28 0.28 0.29 0.29 0.26 0.26\n",
      "recall = 0.05 0.13 0.19 0.22 0.27 0.32 0.39 0.45 0.46 0.52\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y_trn = apply_uniform_noise(Y_trn, 0.4)\n",
    "print('Noise applied')\n",
    "# define the problem\n",
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt_trn, Y_trn, X_feat=X_feat_trn)\n",
    "\n",
    "\n",
    "eurlex4k_train_params.do_fine_tune = True\n",
    "\n",
    "\n",
    "xrt_pretrained = XTransformer.train(\n",
    "    prob,\n",
    "    train_params=eurlex4k_train_params,\n",
    "    pred_params=eurlex4k_pred_params,\n",
    ")\n",
    "\n",
    "# predict and compute metrics\n",
    "P_xrt_pretrained = xrt_pretrained.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_pretrained)\n",
    "\n",
    "print(metrics)\n",
    "model_folder = \"./work_dir/Uni40%\"\n",
    "xrt_pretrained.save(model_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform flipping    \n",
    "noise_level: 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 0.05 0.06 0.11 0.09 0.09 0.09 0.10 0.10 0.11 0.11\n",
      "recall = 0.01 0.02 0.06 0.06 0.08 0.09 0.12 0.14 0.17 0.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y_trn = apply_uniform_noise(Y_trn, 0.6)\n",
    "print('Noise applied')\n",
    "# define the problem\n",
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt_trn, Y_trn, X_feat=X_feat_trn)\n",
    "\n",
    "\n",
    "eurlex4k_train_params.do_fine_tune = True\n",
    "\n",
    "\n",
    "xrt_pretrained = XTransformer.train(\n",
    "    prob,\n",
    "    train_params=eurlex4k_train_params,\n",
    "    pred_params=eurlex4k_pred_params,\n",
    ")\n",
    "\n",
    "# predict and compute metrics\n",
    "P_xrt_pretrained = xrt_pretrained.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_pretrained)\n",
    "\n",
    "print(metrics)\n",
    "model_folder = \"./work_dir/Uni60%\"\n",
    "xrt_pretrained.save(model_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform flipping    \n",
    "noise_level: 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 0.16 0.13 0.16 0.17 0.16 0.15 0.16 0.15 0.15 0.16\n",
      "recall = 0.02 0.04 0.08 0.11 0.13 0.15 0.19 0.21 0.23 0.28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y_trn = apply_uniform_noise(Y_trn, 0.8)\n",
    "print('Noise applied')\n",
    "# define the problem\n",
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt_trn, Y_trn, X_feat=X_feat_trn)\n",
    "\n",
    "\n",
    "eurlex4k_train_params.do_fine_tune = True\n",
    "\n",
    "\n",
    "xrt_pretrained = XTransformer.train(\n",
    "    prob,\n",
    "    train_params=eurlex4k_train_params,\n",
    "    pred_params=eurlex4k_pred_params,\n",
    ")\n",
    "\n",
    "# predict and compute metrics\n",
    "P_xrt_pretrained = xrt_pretrained.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_pretrained)\n",
    "\n",
    "print(metrics)\n",
    "model_folder = \"./work_dir/Uni80%\"\n",
    "xrt_pretrained.save(model_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained by adding Uniform flipping noise data sets is worse than the model trained by adding pair flipping noise data sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8aacf665a889f315b664388628192d2b4a328191a062d046a95d619922fddde3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
